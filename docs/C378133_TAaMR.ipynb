{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C378133 | TAaMR: Targeted Adversarial Attack against Multimedia Recommender Systems",
      "provenance": [],
      "authorship_tag": "ABX9TyNyp1mW//dqdwkaRUWlfoGT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5stnwfPq9Xx"
      },
      "source": [
        "# TAaMR\n",
        "\n",
        "> Targeted Adversarial Attack against Multimedia Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_dSEX2rCDX"
      },
      "source": [
        "## Research Questions\n",
        "\n",
        "1. Can targeted adversarial attacks against images of a low recommended category of products be exploited to modify the recommendation lists of multimedia recommender systems in terms of probability of being more recommended?\n",
        "2. What are the effects of adversarial perturbations against these attacked images for human-perceptions?\n",
        "\n",
        "<p><center><img src='_images/C378133_1.png'></center></p>\n",
        "\n",
        "## Definition of the Targeted Adversarial Attack\n",
        "\n",
        "Let $C$ be a set of classes for a classifier $F$. Let $c \\in C$ be the source class such that $F(x) = c$, and $t \\in C$ be a target class with $t \\neq c$. A Targeted Adversarial Attack finds the adversarial examples $x^âˆ—$ as following:\n",
        "\n",
        "$$\\begin{align*} min_{d\\leq\\epsilon}d(x,x^*)\\\\{such\\ that\\ F(x^*) = t} \\end{align*}$$\n",
        "\n",
        "<p><center><figure><img src='_images/C378133_2.png'><figcaption>Example of a product image before (a) and after (b) a PGD attack ( = 8) against VBPR on Amazon Men.</figcaption></figure></center></p>"
      ]
    }
  ]
}