{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnT1PuhsI8b"
      },
      "source": [
        "# Adversarial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovs_ovTZr7xQ"
      },
      "source": [
        "The objective is to test the robustness of a trained model (target recommender model) against injection attack. We will select a random item(s) (or can ask the user to pick the item) and use the adversarial injection model to learn the fake data. Then we will inject this fake data in the logs of target recommender system. Once the target recommender will get re-trained, we will measure how often it is recommending our target item. Less is better.\n",
        "\n",
        "<p><center><img src='_images/C826054_1.png'></center></p>\n",
        "\n",
        "The attacker will learn fake users through a surrogate model and inject their rating profiles to the training set of the victim model. The attacker will achieve their malicious goal after the victim model consumes these fake users. This suggests that the attack happens at training time of the victim model, instead of test time.\n",
        "\n",
        "The author of this paper formulated the adversarial injection attack as a bi-level optimization problem. \n",
        "\n",
        "<p><center><img src='_images/C826054_2.png'></center></p>\n",
        "\n",
        "The objective will be minimized if normal user’s prediction on target item k is greater than other items, so that the malicious goal of promoting target item will be achieved, as a result.\n",
        "\n",
        "<p><center><img src='_images/C826054_3.png'></center></p>\n",
        "\n",
        "At each iteration t ∈ {1, ...,T } for updating fake data, we first retrain the surrogate model by performing parameter updates for L iterations (line 7). Then we update fake data (line 10) with Projected Gradient Descent (PGD), with ProjΛ(·) as the projection operator that projects the fake data onto feasible set (i.e., $\\hat{x}_{vi}$ ∈ {0, 1} in our case). After the final iteration T where fake data $\\hat{X}^{(T)}$ is learned to minimize $L_{adv}$, they are able to attack the surrogate model, and we hope they can also attack the target (victim) model in a similar way: once trained on the poisoned dataset, it will cause a small adversarial loss $L_{adv}$."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "C826054_Adversarial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
